#!/bin/bash

#############################################################################
#Script Name  : PDA_MASTR_AUTO.sh
#Description  : To automate the execution process of each pda project jobs 
#Args         :
#Author       : Darshankumar Gorasiya
#Email        : x18134751@student.ncirl.ie
#############################################################################

WRKNG_DIR='/home/hduser/project'

# Data Cleaning with Python Script
python3 ${WRKNG_DIR}/py_clean_job/pda_raw_dataClean.py

# Full refresh load to MySQL Server 
mysql --user="mysql" --database="pda" --password="mysql" < ${WRKNG_DIR}/mysql/pda_mysql.sql

# MySql to HDFS load with Sqoop (Existing File Cleaning)
hadoop dfs -rm -R /pda/malware_prediction

# MySql to HDFS load with Sqoop (Load Phase)
sqoop import --connect jdbc:mysql://127.0.0.1/pda --username mysql --password mysql --table malware_prediction --target-dir /pda/malware_prediction
hadoop dfs -rm -R /pda/malware_prediction/_SUCCESS

# MapReduce Task1, Shell Call (Rest mapreduce related code and file managment is done at RunMapReduceShell.sh script level.)
sh ${WRKNG_DIR}/mapreduce_task1/RunMapReduceShell.sh

# MapReduce Task2, Shell Call (Rest mapreduce related code and file managment is done at RunMapReduceShell.sh script level.)
sh ${WRKNG_DIR}/mapreduce_task2/RunMapReduceShell.sh

# Pig Target HDFS Directory Cleanup
hdfs dfs -rm -R /pda/result/pig_query3_by
hdfs dfs -rm -R /pda/result/pig_query7_by

# Call for Pig Jobs execution on Distributed Mode
pig -x mapreduce ${WRKNG_DIR}/pig/pda_malpred_q3q7.pig

# Load Pig result data from HDFS to Hbase
hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator="," -Dimporttsv.columns=HBASE_ROW_KEY,cf2 pig_query3_by hdfs://localhost:54310/pda/result/pig_query3_by/part-r-00000

hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator="," -Dimporttsv.columns=HBASE_ROW_KEY,cf2 pig_query7_by hdfs://localhost:54310/pda/result/pig_query7_by/part-r-00000

# Hive execution and Loading results back to HDFS
hive -f ${WRKNG_DIR}/hive/pda_hive_job.hql

# SparkSQL job execution and result load script execution
# (Pre-Execution - Cleanup)
hdfs dfs -rm -R /pda/SparkSQLOut/spark_df
# (Execution Phase and Load automation script call)
spark-submit ${WRKNG_DIR}/sparksql/pda_sparkSQL_job.py
